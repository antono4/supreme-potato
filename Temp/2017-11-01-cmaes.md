---
layout: post
title: CMA-ES - From Preliminaries to Source Code
tags: [Evolutionary Optimization, Evolutionary Algorithms, Evolutionary Strategies, CMA-ES, Genetic Algorithms, Swarm Optimization]
---

<style>
body {
text-align: justify}
</style>

Little helpers:

$$x = {-b \pm \sqrt{b^2-4ac} \over 2a} $$


$$
\mathbf{V}_1 \times \mathbf{V}_2 =  \begin{vmatrix} 
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
\frac{\partial X}{\partial u} &  \frac{\partial Y}{\partial u} & 0 \\
\frac{\partial X}{\partial v} &  \frac{\partial Y}{\partial v} & 0 \\
\end{vmatrix}
$$


## [1] Motivation 

Optimization is an important concept in mathematics and related fields. The idea is to maximize (or minimize) a certain desired criteria (function, specifically called objective function or goal function) while satisfying the relevant restrictions (called constraints), if there are any. Optimization is common in traditional engineering industries, decision making, operations research, machine/deep learning, artificial intelligence and many more, making it almost ubiquitous. 

For complex optimization problems, the global landscape of the objective function is rugged, multi-modal and patchy, where feasible regions are separated by unfeasible regions in the design variable space. The landscape becomes more complex as the dimensionality of the problem increases. Gradient based methods terminate at local minimum and unless the objective function is convex (for a minimization problem), will therefore converge to a local optimum. In problems with complex objective function landscape, we therefore need gradient free search algorithms that possess a global search capability.  

Optimization problems where one can numerically obtains the influence of the affecting (decision) variables on the value of the objective function, even in the absence of some analytic algebraic model of the system, are well suited to be solved through gradient free population based optimization metaheuristic techniques, popularly known as *evolutionary algorithms*. The search procedure of these algorithms uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination and selection. One begins with an initial pool of guess solutions. This pool of solutions is referred as *population*  and the *population size* indicates the number of candidate solutions in the population. Based on the algorithmic rules, the population members explore the search space and evolve through *generations* by exchanging information between fellow members. With an appropriate implementation, one can expect the population members to converge towards the region where the mean value of the objective function is low. Convergence of the population towards an extremely low mean objective function value suggests that the algorithm approaches or approximates the global optimum. 

There are several classes of evolution(or related)-inspired search algorithms: generic algorithms, genetic programming, swarm algorithms, ant-colony optimization, evolutionary strategies etc., and numerous variants of these. The choice of the algorithms is often based on the complexity of the problem. No single algorithm is usually capable to solve all types of problems. That being said evolutionary strategies represent the current state-of-the-art algorithms. The most impressive algorithm of this class is the *covariance matrix adaptation evolutionary strategy* or **CMA-ES**. This tutorial is aimed at presenting a detailed understanding of CMA-ES to the reader. The attempt is to begin from the very basic building blocks of the algorithm all the way to the source code in Python.

## [2] Preliminaries

We begin with some preliminary concepts that prove useful for later on purposes. We will first have some high level discussion of the eigen decomposition of a square matrix and discuss some key concepts of a multivariate normal distribution.  

### [2.1] Eigendecomposition of a square matrix
Eigenvectors and eigenvalues are vectors and numbers associated with **square matrices**, and together they provide the eigendecomposition of a matrix. Eigendecomposition does not exists for all the square matrices. It has a convenient expression for a class of matrices that are very frequently used in mutivariate analysis: correlation, covariance etc. matrices. A vectors **b** is an eigenvector of a square matrix **C** if: 


​                                                       **Cb** = $$\lambda$$**b**

> A neat way of putting this is as follows: vector **b** is an eigenvector of **C** if upon multiplication with **C**, it is **unrotated**. It can either change in magnitude or orient in an exact opposite direction (which is not considered a 'rotation').

As an example, consider a matrix **C**:


$$
\mathbf{C} =  \begin{bmatrix} 
2 & 3  \\
2 &  1 
\end{bmatrix}
$$


The eigenvectors corresponding to **C** are given by:
$$
\mathbf{b_1} =  \begin{bmatrix} 
3  \\
2  
\end{bmatrix},  \text{and}\:\: \mathbf{b_2} =  \begin{bmatrix} 
-1  \\
 \:\:1  
\end{bmatrix}, \text{with the respective eigenvalues as:}
\: \lambda_1 = 4 \:\text{and}\: \lambda_2 = -1
$$
The method of calculating the eigenvectors only requires high school mathematical  skills. A detailed source can be found [here](http://lpsa.swarthmore.edu/MtrxVibe/EigMat/MatrixEigen.html). One can see that any scalar multiple  of an eigenvector still is an eigenvector. Thus, strictly speaking, there is an infinity of eigenvectors associated with an eigenvalue of a matrix. For most applications, **we therefore normalize the eigenvectors while reporting**. Enough said. Traditionally, we put together the set of eigenvectors of **A** in a matrix denoted by **B**. Each column of **B** is an eigenvector of **C**. The eigenvalues are stored in a diagonal matrix denoted by $$\Lambda$$, where the diagonal elements gives the eigenvalues:

  
$$
\textbf{CB} = \textbf{C}\Lambda \tag{1}
$$

### [2.2] Positive semi-definite matrices

Informally, matrices are called positive semi definite when:

* All the eigenvalues are either positive or null.
* Eigenvectors are composed of real numbers and are pairwise orthonormal. This property is a direct result of the fact that positive definite matrices are symmetric. Proof can be found [elsewhere](https://math.stackexchange.com/questions/82467/eigenvectors-of-real-symmetric-matrices-are-orthogonal).

> Does orthonormal and orthogonal sounds confusing ? Two vectors *u, v* are orthogonal if their inner product is zero. In other words <u,v> = 0. They are orthonormal if they are orthogonal but each vector has norm 1. In other words <u,v> = 0 but also, <u,u> = <v,v> = 1. Hope that the normalization argument of an eigenvector previously talked about makes more sense now.

If the eigenvectors are pairwise orthonormal, $$\textbf{BB}^{\text{T}} = \textbf{I}$$, or $$\textbf{B}^\text{-1} = \textbf{B}^{\text{T}}$$ . We can extend (1) as:


$$
\textbf{CB}\textbf{B}^{-1} = \textbf{B}\Lambda\textbf{B}^{-1}
$$


For a positive semi definite matrix:


$$
\textbf{C} = \textbf{B}\Lambda\textbf{B}^{-1}, \:\text{or}
$$

$$
\textbf{C} = \textbf{B}\Lambda\textbf{B}^{\text{T}} \tag{2}
$$



(2) represents the eigendecomposition of a positive semi definite matrix. 



### [2.3] Positive definite matrices

Positive definite matrices represent a special case of positive semi-definite matrices. *All the eigenvalues of a positive definite matrix are positive.* Formally, a symmetric matrix ***C*** $$\in$$ $$R^{n \times n}$$ is positive definite if for all *x* $$\in$$ $$R^{n}$$\{0}: $$x^{\text{T}}Cx$$ > 0. As with the case of positive semi definite, ***C*** has orthonormal basis of eigenvectors, ***B*** = [$$b_1, b_2.....b_n$$], with corresponding eigenvalues $$d_1^2, ......d_n^2$$ > 0. 



The eigendecomposition of ***C*** obeys (arrives from (2)):
$$
\boldsymbol{C} = \boldsymbol{BD}^{\text{2}}\boldsymbol{B}^{\text{T}}  \tag{3}
$$
where

* ***B*** is an *orthogonal matrix*. $$\boldsymbol{BB}^{\text{T}}$$ =$$\boldsymbol{B}^{\text{T}}\boldsymbol{B}$$ = $$\textbf{I}$$ 
* $$\boldsymbol{D}^{\text{2}}$$ = diag($$d_1^2, ......d_n^2$$) is a diagonal matrix with eigenvalues of $$\boldsymbol{C}$$ as diagonal elements.

From (3), one can see that:
$$
\boldsymbol{C^{\text{1/2}}} = \boldsymbol{BD}\boldsymbol{B}^{\text{T}}  \tag{4}
$$


Also,
$$
\boldsymbol{C^{\text{ -1/2}}} = \boldsymbol{BD}{^{-1}}\boldsymbol{B}^{\text{T}}  =  \boldsymbol{B}\:\text{diag}(1/d_1, ......1/d_n)\boldsymbol{B}^{\text{T}} \tag{5}
$$


,and
$$
\boldsymbol{C^{\text{ -1}}} =   \boldsymbol{B}\:\text{diag}(1/d_1^{2}, ......1/d_n^{2})\boldsymbol{B}^{\text{T}} \tag{6}
$$
Equation (4), (5), and (6) may take some time to appear convincing. Article at [wikipedia](https://en.wikipedia.org/wiki/Matrix_multiplication#Properties_of_the_matrix_product_.28two_matrices.29) is helpful.



## [3] The multivariate normal distribution

the multivariate normal distribution concerns with the normal distribution in all the 